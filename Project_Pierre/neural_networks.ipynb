{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labels and text\n",
    "labels, texts = [], []\n",
    "data_neg = open('data/train_neg_full.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    texts.append(line)\n",
    "    labels.append(-1)\n",
    "data_pos = open('data/train_pos_full.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_pos.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    texts.append(line)\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sets, load embeddings and tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and validation sets\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1193514, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We convert from glove to word2vect\n",
    "glove2word2vec(glove_input_file='data/glove.twitter.27B.25d.txt', word2vec_output_file='data/glove.twitter.27B.25d_word2vect.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-99ed47fc8cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the embeddings vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/glove.twitter.27B.25d_word2vect.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the embeddings vectors\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/glove.twitter.27B.25d_word2vect.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tokenizer\n",
    "token = text.Tokenizer()\n",
    "token.fit_on_texts(trainDF['text'])\n",
    "word_index = token.word_index\n",
    "\n",
    "# Convert text to sequence of tokens and pad them to ensure equal length vectors \n",
    "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
    "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create token-embedding mapping\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 25))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        embedding_vector = model.get_vector(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural net implementations\n",
    "\n",
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    cnn_model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    cnn_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classifier = create_cnn()\n",
    "cnn_classifier.fit(train_seq_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_predictions = cnn_classifier.predict(valid_seq_x)\n",
    "\n",
    "cnn_predictions[cnn_predictions >= 0.5] = 1\n",
    "cnn_predictions[cnn_predictions < 0.5] = 0\n",
    "\n",
    "valid_percent = 100 * (1 - np.count_nonzero(np.array([valid_y]).T - cnn_predictions.astype(int)) / len(cnn_predictions))\n",
    "print('local results: ' + str(valid_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission code\n",
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "cnn_test_predictions = cnn_classifier.predict(test_seq_x)\n",
    "test_predictions = np.round(cnn_test_predictions).astype(int)\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('CNNSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_lstm():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    lstm_model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    lstm_model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_classifier = create_rnn_lstm()\n",
    "\n",
    "lstm_classifier.fit(train_seq_x, train_y, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions = lstm_classifier.predict(valid_seq_x)\n",
    "\n",
    "lstm_predictions[lstm_predictions >= 0.5] = 1\n",
    "lstm_predictions[lstm_predictions < 0.5] = 0\n",
    "\n",
    "valid_percent = 100 * (1 - np.count_nonzero(np.array([valid_y]).T - lstm_predictions.astype(int)) / len(lstm_predictions))\n",
    "print('local results: ' + str(valid_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission code\n",
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "lstm_test_predictions = lstm_classifier.predict(test_seq_x)\n",
    "test_predictions = np.round(lstm_test_predictions).astype(int)\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('LSTMSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_gru():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the GRU Layer\n",
    "    gru_layer = layers.GRU(100)(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(gru_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_classifier = create_rnn_gru()\n",
    "\n",
    "gru_classifier.fit(train_seq_x, train_y, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_predictions = gru_classifier.predict(valid_seq_x)\n",
    "\n",
    "gru_predictions[gru_predictions >= 0.5] = 1\n",
    "gru_predictions[gru_predictions < 0.5] = 0\n",
    "\n",
    "valid_percent = 100 * (1 - np.count_nonzero(np.array([valid_y]).T - gru_predictions.astype(int)) / len(gru_predictions))\n",
    "print('local results: ' + str(valid_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv\n",
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "gru_test_predictions = gru_classifier.predict(test_seq_x)\n",
    "test_predictions = np.round(gru_test_predictions).astype(int)\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('GRUSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_rnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "\n",
    "    # Add the LSTM Layer\n",
    "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_classifier = create_bidirectional_rnn()\n",
    "\n",
    "bidirectional_classifier.fit(train_seq_x, train_y, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidirectional_predictions = bidirectional_classifier.predict(valid_seq_x)\n",
    "\n",
    "bidirectional_predictions[bidirectional_predictions >= 0.5] = 1\n",
    "bidirectional_predictions[bidirectional_predictions < 0.5] = 0\n",
    "\n",
    "valid_percent = 100 * (1 - np.count_nonzero(np.array([valid_y]).T - bidirectional_predictions.astype(int)) / len(bidirectional_predictions))\n",
    "print('local results: ' + str(valid_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv\n",
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "bidirectional_test_predictions = bidirectional_classifier.predict(test_seq_x)\n",
    "test_predictions = np.round(bidirectional_test_predictions).astype(int)\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('BIDIRECTIONALSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rcnn():\n",
    "    # Add an Input Layer\n",
    "    input_layer = layers.Input((70, ))\n",
    "\n",
    "    # Add the word embedding Layer\n",
    "    embedding_layer = layers.Embedding(len(word_index) + 1, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=True)(input_layer)\n",
    "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
    "    \n",
    "    # Add the recurrent layer\n",
    "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
    "    \n",
    "    # Add the convolutional Layer\n",
    "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
    "\n",
    "    # Add the pooling Layer\n",
    "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
    "\n",
    "    # Add the output Layers\n",
    "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
    "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
    "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
    "\n",
    "    # Compile the model\n",
    "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
    "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_classifier = create_rcnn()\n",
    "\n",
    "rcnn_classifier.fit(train_seq_x, train_y, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcnn_predictions = rcnn_classifier.predict(valid_seq_x)\n",
    "\n",
    "rcnn_predictions[rcnn_predictions >= 0.5] = 1\n",
    "rcnn_predictions[rcnn_predictions < 0.5] = 0\n",
    "\n",
    "valid_percent = 100 * (1 - np.count_nonzero(np.array([valid_y]).T - rcnn_predictions.astype(int)) / len(rcnn_predictions))\n",
    "print('local results: ' + str(valid_percent) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate csv\n",
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "rcnn_test_predictions = rcnn_classifier.predict(test_seq_x)\n",
    "test_predictions = np.round(rcnn_test_predictions).astype(int)\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('RCNNSubmission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_out = np.hstack((cnn_predictions, lstm_predictions, gru_predictions, bidirectional_predictions, rcnn_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV(Cs=5, cv = 5, n_jobs = 1, multi_class = 'ovr').fit(nn_out, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "lstm_test_predictions = lstm_classifier.predict(test_seq_x)\n",
    "cnn_test_predictions = cnn_classifier.predict(test_seq_x)\n",
    "gru_test_predictions = gru_classifier.predict(test_seq_x)\n",
    "rnn_test_predictions = bidirectional_classifier.predict(test_seq_x)\n",
    "rcnn_test_predictions = rcnn_classifier.predict(test_seq_x)\n",
    "\n",
    "test_nn_out = np.hstack((cnn_test_predictions, lstm_test_predictions, gru_test_predictions, rnn_test_predictions, rcnn_test_predictions))\n",
    "test_predictions = clf.predict(test_nn_out)\n",
    "\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('Logistic_CNN_LSTM_GRU_RNN_RCNN_Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snn(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ))\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_classifier = create_snn(nn_out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_classifier.fit(nn_out, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CNN csv\n",
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "lstm_test_predictions = lstm_classifier.predict(test_seq_x)\n",
    "cnn_test_predictions = cnn_classifier.predict(test_seq_x)\n",
    "gru_test_predictions = gru_classifier.predict(test_seq_x)\n",
    "rnn_test_predictions = bidirectional_classifier.predict(test_seq_x)\n",
    "rcnn_test_predictions = rcnn_classifier.predict(test_seq_x)\n",
    "\n",
    "test_nn_out = np.hstack((cnn_test_predictions, lstm_test_predictions, gru_test_predictions, rnn_test_predictions, rcnn_test_predictions))\n",
    "test_predictions = snn_classifier.predict(test_nn_out)\n",
    "test_predictions = np.round(test_predictions).astype(int)\n",
    "\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('SNN_CNN_LSTM_GRU_RNN_RCNN_Submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "data_neg = open('data/test_data.txt', encoding=\"utf8\").read()\n",
    "for i, line in enumerate(data_neg.split('\\n')):\n",
    "    if (line == ''):\n",
    "        break\n",
    "    test_texts.append(line)\n",
    "\n",
    "testDF = pd.DataFrame()\n",
    "testDF['text'] = test_texts\n",
    "test_seq_x = sequence.pad_sequences(token.texts_to_sequences(testDF['text']), maxlen=70)\n",
    "\n",
    "lstm_test_predictions = lstm_classifier.predict(test_seq_x)\n",
    "cnn_test_predictions = cnn_classifier.predict(test_seq_x)\n",
    "gru_test_predictions = gru_classifier.predict(test_seq_x)\n",
    "rnn_test_predictions = bidirectional_classifier.predict(test_seq_x)\n",
    "rcnn_test_predictions = rcnn_classifier.predict(test_seq_x)\n",
    "\n",
    "test_pred_matrix = np.round(np.hstack((cnn_test_predictions, lstm_test_predictions, gru_test_predictions, rnn_test_predictions, rcnn_test_predictions))).astype(int)\n",
    "test_predictions = test_pred_matrix.sum(axis=1)\n",
    "test_predictions[test_predictions < 2.5] = 0\n",
    "test_predictions[test_predictions >= 2.5] = 1\n",
    "\n",
    "test_predictions[test_predictions == 0] = -1\n",
    "testDF['Prediction'] = test_predictions\n",
    "testDF = testDF.drop(['text'], axis=1)\n",
    "testDF['Id'] = testDF.index + 1\n",
    "testDF = testDF.set_index('Id')\n",
    "testDF.to_csv('Majority_CNN_LSTM_GRU_RNN_RCNN_Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
